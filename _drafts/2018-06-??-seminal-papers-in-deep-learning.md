---

---

seminal papers on deep learning:

1. <a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank">DCGAN</a> Radford 2015. Unsupervised learning, image representation
2. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank">CNN</a> LeCun, 1998, classification
3. <a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank">GAN</a> Goodfellow, 2014, 
4. RNN
5. <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank">Adam Optimzer</a>
6. <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank">Batch normalization</a>



Keywords: 

Autoencoders, clustering, (un)supervised learning of unlabled data, maxpooling, strided convolution, fully connected layers, dropout, stochoastic gradient descent, log probability, regularization, MLE, Boltzmann machine, latent variable, batch normalization (i.e. normalize input to each unit to have zero mean and unit variance), ReLU activation, leaky ReLU, sigmoid function, Tanh function, softmax, maxout, mini-batch, momentum, hyperparameters, 

ML types: 

Linear / logistic regression, SVM, 

Constraints are:
1. Computational speed
2. Architecture complexity
3. ...

## What does it mean when people say deep learning feel more like an art than a science? 

## Available datasets (that I'm aware of) for deep neural network training
* MNIST
* Faces
* MegaFace
* CIFAR-10
* ImageNet-1k
* Large-scale Scene Understanding


