I"°<p>exploration is gathering information, and exploitation is using the information you have to get a known good result.</p>

<p>In computer science, the tension between exploration and exploitation takes its most concrete form in a scenario called the ‚Äúmulti-armed bandit problem.‚Äù The problem in the most general form: multiple options to pursue, a different probability of reward for each option, and a certain amount of effort (or money, or time) to be allocated among them.</p>

<p>Imagine walking into a casino full of different slot machines, each one with its own odds of a payoff. The rub, of course, is that you aren‚Äôt told those odds in advance: until you start playing, you won‚Äôt have any idea which machines are the most lucrative (‚Äúloose,‚Äù as slot-machine aficionados call it) and which ones are just money sinks. Naturally, you‚Äôre interested in maximizing your total winnings. And it‚Äôs clear that this is going to involve some combination of pulling the arms on different machines to test them out (exploring), and favoring the most promising machines you‚Äôve found (exploiting).</p>

<p>A sobering property of trying new things is that the value of exploration, of finding a new favorite, can only go down over time. The flip side is that the value of exploitation can only go up over time. So explore when you will have time to use the resulting knowledge, exploit when you‚Äôre ready to cash in. The interval makes the strategy.</p>

<p>Win-Stay, Lose-Shift algorithm: choose an arm at random, and keep pulling it as long as it keeps paying off.</p>

<p>Economists refer to this idea, of valuing the present more highly than the future, as ‚Äúdiscounting.‚Äù</p>

<p>Gittins index: For every slot machine we know little or nothing about, there is some guaranteed payout rate which, if offered to us in lieu of that machine, will make us quite content never to pull its handle again. It‚Äôs based on geometric discounting of future reward, valuing each pull at a constant fraction of the previous one. The Gittins index, then, provides a formal, rigorous justification for preferring the unknown, provided we have some opportunity to exploit the results of what we learn from exploring. The unknown has a chance of being better, even if we actually expect it to be no different, or if it‚Äôs just as likely to be worse.</p>

<p>Regret can be highly motivating. regret minimization framework. In the long run, optimism is the best prevention for regret.</p>

<p>Upper Confidence Bound algorithms. In a multi-armed bandit problem, an Upper Confidence Bound algorithm says, quite simply, to pick the option for which the top of the confidence interval is highest. So an Upper Confidence Bound algorithm doesn‚Äôt care which arm has performed best so far; instead, it chooses the arm that could reasonably perform best in the future. the Upper Confidence Bound is always greater than the expected value, but by less and less as we gain more experience with a particular option.</p>

<p>Upper Confidence Bound algorithms implement a principle that has been dubbed ‚Äúoptimism in the face of uncertainty.‚Äù Optimism, they show, can be perfectly rational. By focusing on the best that an option could be, given the evidence obtained so far, these algorithms give a boost to possibilities we know less about.</p>

<p>Bandits live (A/B testing)</p>

<p>In general, it seems that people tend to over-explore‚Äîto favor the new disproportionately over the best.</p>
:ET