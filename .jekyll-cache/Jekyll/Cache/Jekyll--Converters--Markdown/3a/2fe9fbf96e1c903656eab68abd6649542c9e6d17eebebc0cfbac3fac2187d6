I"∫%<p>For newcomers to data science and machine learning (ML) like myself, the variety and the depth of different topics can be overwhelming at times. So in this post I document various notes as I learn about different facets of machiine learning. The notes are very condensed, and the intent is to use them as references in actual practice. So for total beginners this post will probably not be very helpful. Hopefully this will benefit not only myself but others out there as well. Briefly, this post contains the following:</p>

<ul>
  <li><a href="#machine-learning-workflow"><strong>ML workflow</strong></a>. From data preprocessing, missing data handling, to training and validation strategies.</li>
  <li><a href="#common-ml-libraries-and-methods"><strong>ML libraries and methods</strong></a>. References to common methods in pandas, scikit-learn and seaborn.</li>
  <li><a href="#variable-types"><strong>Variable types</strong></a></li>
  <li><a href="#miscellaneous-notes"><strong>Miscellaneous notes</strong></a></li>
</ul>

<h2 id="machine-learning-workflow">Machine learning workflow</h2>

<h1 id="data-wrangling--munging--pre-processing">Data wrangling / munging / pre-processing</h1>
<ol>
  <li><strong>Data exploration.</strong> Get a bird eye overview of the available data (how many entries? what type of values is each feature represented? ). Interpret the physical meaning of each feature (some domain knowledge will be, check data structure (i.e. what type of value is in each feature?), use visualization techniques (e.g. <code class="highlighter-rouge">box</code>, <code class="highlighter-rouge">scatter</code>, <code class="highlighter-rouge">hist</code> or <code class="highlighter-rouge">heatmap</code>) grasp univariate, bivariate or multivariate relationships. Understand the value distributions in the variable (is it normally distributed or skewed? What‚Äôs the relationship between independent and dependent variables?)</li>
  <li><strong>Missing values.</strong> Fill in missing values by (1) dropping the columns, (2) imputing the missing values, or (3) adding new duplicate columns with imputated values filled in. There are also variations on how you decide to fill the missing values. Is it with the median, mean, mode, or some other value.</li>
  <li><strong>Data encoding.</strong> Encode/create dummy variables for categorical data, using one-hot encoding, or completely drop the columns.</li>
  <li><strong>Feature engineering.</strong> To get the best model performance, the feature values should be normally distributed, with mean and deviation normalized. Therefore, often times some feature engineering is required to get the features into their optimal forms.</li>
</ol>

<h1 id="training-and-prediction">Training and prediction</h1>
<ol>
  <li><strong>Pre-training preparations.</strong> Split data into training, validation, and testing sets. Choose which model to best train on the data</li>
  <li><strong>Training.</strong> Choose a base model (or multiple ones by stacking them, e.g. random forest, deicision tree, XGBoost, etc). Train features to fit to target data</li>
  <li><strong>Validation.</strong> Evaluate training results by predicting validation targets</li>
  <li><strong>Refinement.</strong> Iteratively improve model, avoid under/over fitting</li>
</ol>

<h2 id="common-ml-libraries-and-methods">Common ML libraries and methods</h2>
<h1 id="pandas">pandas</h1>
<p><strong>Useful methods</strong></p>
<ul>
  <li><code class="highlighter-rouge">describe()</code></li>
  <li><code class="highlighter-rouge">dtypes</code></li>
  <li><code class="highlighter-rouge">get_dummies()</code></li>
  <li><code class="highlighter-rouge">align</code></li>
  <li><code class="highlighter-rouge">value_counts()</code></li>
  <li><code class="highlighter-rouge">groupby</code></li>
  <li><code class="highlighter-rouge">rename</code></li>
  <li><code class="highlighter-rouge">concat</code>, <code class="highlighter-rouge">join</code>, <code class="highlighter-rouge">merge</code></li>
</ul>

<p><strong>Univariate and bivariate plotting with pandas</strong></p>
<ul>
  <li><code class="highlighter-rouge">bar</code> (+stacking), <code class="highlighter-rouge">line</code> (+stacking), <code class="highlighter-rouge">area</code> (+stacking) charts and histogram</li>
  <li><code class="highlighter-rouge">scatter</code>, <code class="highlighter-rouge">hexbin</code> (+stacking)</li>
</ul>

<h1 id="sklearn">sklearn</h1>

<ul>
  <li>.tree: <code class="highlighter-rouge">DecisionTreeRegressor</code></li>
  <li>.linear_model: <code class="highlighter-rouge">ElasticNet</code>, <code class="highlighter-rouge">Lasso</code>, <code class="highlighter-rouge">BayesianRidge</code>, <code class="highlighter-rouge">LassoLarsIC</code></li>
  <li>.kernel_ridge: <code class="highlighter-rouge">KernelRidge</code></li>
  <li>.lightgbm (as lgb): <code class="highlighter-rouge">LGBMRegressor</code></li>
  <li>.xgboost (as xgb): <code class="highlighter-rouge">XGBRegressor</code> (dominant algorithm for tabular/structured data)</li>
  <li>.ensemble: <code class="highlighter-rouge">RandomForestRegressor</code>, <code class="highlighter-rouge">GradientBoostingRegressor</code></li>
  <li>.model_selection: <code class="highlighter-rouge">cross_val_score</code>, <code class="highlighter-rouge">tran_test_split</code>, <code class="highlighter-rouge">KFold</code></li>
  <li>.impute: <code class="highlighter-rouge">SimpleImputer</code> (for missing values)</li>
  <li>.preprocessing: <code class="highlighter-rouge">Imputer</code> (for missing values), <code class="highlighter-rouge">LabelEncoder</code> (for categorical values), <code class="highlighter-rouge">RobustScaler</code></li>
  <li>.ensemble.partial_dependence: <code class="highlighter-rouge">partial_dependence</code>, <code class="highlighter-rouge">plot_partial_dependence</code></li>
  <li>
    <p>.pipeline: <code class="highlighter-rouge">make_pipeline</code></p>
  </li>
  <li>
    <p>.metrics: <code class="highlighter-rouge">mean_absolute_error</code>, <code class="highlighter-rouge">mean_squared_error</code></p>
  </li>
  <li>Model tuning variables: n_estimators, early_stopping_rounds, learning_rate</li>
</ul>

<h1 id="seaborn">seaborn</h1>
<p><strong>Plotting</strong></p>
<ul>
  <li><code class="highlighter-rouge">countplot</code> (like <code class="highlighter-rouge">bar</code>), <code class="highlighter-rouge">kdeplot</code> (like <code class="highlighter-rouge">line</code>), <code class="highlighter-rouge">distplot</code> (like <code class="highlighter-rouge">hist</code>), <code class="highlighter-rouge">jointplot</code> (bivariate, like <code class="highlighter-rouge">scatter</code> and <code class="highlighter-rouge">hex</code>), <code class="highlighter-rouge">boxplot</code>, <code class="highlighter-rouge">violinplot</code></li>
  <li><code class="highlighter-rouge">FacetGrid</code>, <code class="highlighter-rouge">pairplot</code></li>
  <li><code class="highlighter-rouge">lmplot</code></li>
  <li><code class="highlighter-rouge">heatmap</code> (use withi .corr() on value pairs)</li>
  <li>pandas.plotting.parallel_coordinates</li>
</ul>

<h1 id="sciply">sciply</h1>
<ul>
  <li>.stats: <code class="highlighter-rouge">probplot</code></li>
</ul>

<h2 id="variable-types">Variable types</h2>
<p>Most of the tabular datasets will contain values with one of the follow flavors:</p>
<ul>
  <li><strong>ordinal</strong> - positional value - ordinal scale</li>
  <li><strong>interval</strong> - continuous, with relationships</li>
  <li><strong>categorical</strong> - nominal scale</li>
  <li><strong>nominal</strong> - numbers used only as a name, no intrinsic hierarchical values (inherently categorical)</li>
</ul>

<h2 id="encoding-variables">Encoding variables</h2>
<ol>
  <li><strong>Find and replace:</strong> e.g. convert ‚Äúone‚Äù to ‚Äú1‚Äù. Create library of replacement terms, then use <code class="highlighter-rouge">.replace()</code></li>
  <li><strong>Label encoding:</strong> first use <code class="highlighter-rouge">.astype('category')</code> to convert feature into categorical variables, then create a new column with the numerical mappings useing <code class="highlighter-rouge">.cat.codes</code>. The mapped values do not represent importance of the value, but this could mislead the algorithm into thinking one is more important than the other.</li>
  <li><strong>One hot encoding:</strong> use <code class="highlighter-rouge">pd.get_dummies()</code> to add additional columns with binary values reflecting each possible choice in the feature. This could result in many new columns, so be careful.</li>
  <li><strong>Custom binary encoding:</strong> use domain knowledge, you can group a selection of values into one since they have the same significance as far as the model is concerned.</li>
</ol>

<h2 id="miscellaneous-notes">Miscellaneous notes</h2>
<ul>
  <li>Perform cross-validation on <strong>small</strong> datasets to get higher quality model, use train-test split on <strong>big</strong> datasets to save time. To determine whether a dataset is big or small, you can run a cross-validation test and see if scores between experiments are close. If they are, then cross-validation probably doesn‚Äôt do much.</li>
  <li>Data leakage: any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won‚Äôt be available to the model. Also, training and validation data should not intermix to avoid contaminating model fitting process.</li>
  <li>Model stacking (a.k.a. meta ensembling)</li>
</ul>

:ET