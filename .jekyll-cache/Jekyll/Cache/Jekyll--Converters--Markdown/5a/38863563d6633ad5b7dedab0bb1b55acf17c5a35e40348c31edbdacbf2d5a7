I"”<p>During my job search, I often ask myself this question: how much time should I spend to perfect my resume versus networking and actively applying to jobs? In an ideal world with infinite time and resources (and employers who don‚Äôt care about prolonged vacancies in experiences!), I could start the process only when I have the perfect resume full of completed projects and experiences, but unfortunately I live in a time- and resource-constrained world that is always forces me to have to make a choice. If I spend too much time honing my skills instead of applying, then companies won‚Äôt know that I‚Äôm open to opportunities. On the other hand, if I only apply to jobs without leveling up my skills, then my resume will not be competitive against other candidates. So what <em>is</em> the optimal way to approach this dilemma? How can I maximize my chance of landing interviews and getting a job?</p>

<!--more-->

<p>This tension between exploring new skills and exploiting past experiences is actually studied extensively in mathematics, in what‚Äôs called the <strong>explore-exploit trade-off</strong>, where exploration involves gathering information, and exploitation uses the information available to get a known good result. This is more commonly known as the <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit problem</a>. The problem is described in the book <a href="https://algorithmstoliveby.com/">Algorithms to Live By</a> as follows.</p>

<blockquote>
  <p>Imagine walking into a casino full of different slot machines, each one with its own odds of a payoff. The rub, of course, is that you aren‚Äôt told those odds in advance: until you start playing, you won‚Äôt have any idea which machines are the most lucrative (‚Äúloose,‚Äù as slot-machine aficionados call it) and which ones are just money sinks. Naturally, you‚Äôre interested in maximizing your total winnings. And it‚Äôs clear that this is going to involve some combination of pulling the arms on different machines to test them out (exploring), and favoring the most promising machines you‚Äôve found (exploiting).</p>
</blockquote>

<p>In general, the problem states that a person is faced with multiple options (level up skills or network and apply to jobs), a different probability of reward for each option (each choice contributes to finding a job with varying success), and a certain amount of effort (or money, or time) to be allocated among them.</p>

<p>It‚Äôs important to note that the value of learning new skills (‚Äúexploration‚Äù) will diminish over time. The flip side is that the value of applying to jobs (‚Äúexploitation‚Äù) will go up over time (eventually everyone will get a job right? <em>Right?</em>). The math says that the optimal strategy depends on much time is left. <strong>So explore when you will have time to use the resulting knowledge, exploit when you‚Äôre ready to cash in.</strong></p>

<p>One strategy is to learn some new skill you think will help you get a job and demonstrate that to an employer, and if it lands you interviews, then as long as it gets you interviews keep selling that as your main competitive advantage. This is the so-called <strong>Win-Stay, Lose-Shift algorithm</strong>.</p>

<p>When not knowing the interval to consider, the present has a higher priority: a cured patient today is taken to be more valuable than one cured a week or a year from now, and certainly the same holds true of profits. Economists refer to this idea, of valuing the present more highly than the future, as ‚Äúdiscounting.‚Äù</p>

<p><strong>Gittins index</strong>: Pay-off maximization framework. For every slot machine we know little or nothing about, there is some guaranteed payout rate which, if offered to us in lieu of that machine, will make us quite content never to pull its handle again. It‚Äôs based on geometric discounting of future reward, valuing each pull at a constant fraction of the previous one. The Gittins index, then, provides a formal, rigorous justification for preferring the unknown, provided we have some opportunity to exploit the results of what we learn from exploring. The unknown has a chance of being better, even if we actually expect it to be no different, or if it‚Äôs just as likely to be worse.</p>

<p>Gittins index is not quick to compute, so another framework would be more accessible in general cases. Regret can be highly motivating. regret minimization framework. In the long run, optimism is the best prevention for regret.</p>

<p><strong>Upper Confidence Bound algorithms</strong>: In a multi-armed bandit problem, an Upper Confidence Bound algorithm says, quite simply, to pick the option for which the top of the confidence interval is highest. So an Upper Confidence Bound algorithm doesn‚Äôt care which arm has performed best so far; instead, it chooses the arm that could reasonably perform best in the future. the Upper Confidence Bound is always greater than the expected value, but by less and less as we gain more experience with a particular option.</p>

<p>Upper Confidence Bound algorithms implement a principle that has been dubbed ‚Äúoptimism in the face of uncertainty.‚Äù Optimism, they show, can be perfectly rational. By focusing on the best that an option could be, given the evidence obtained so far, these algorithms give a boost to possibilities we know less about.</p>

<p>Bandits live (A/B testing)</p>

<p>In general, it seems that people tend to over-explore to favor the new disproportionately over the best. when the world can change, continuing to explore can be the right choice. So long as things continue to change, you must never fully cease exploring.</p>

<p>The Gittins index and the Upper Confidence Bound, as we‚Äôve seen, inflate the appeal of lesser-known options beyond what we actually expect, since pleasant surprises can pay off many times over. But at the same time, this means that exploration necessarily leads to being let down on most occasions. Shifting the bulk of one‚Äôs attention to one‚Äôs favorite things should increase quality of life.</p>
:ET